{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "730c3071",
   "metadata": {},
   "source": [
    "## NEURALNETWORKSANDDEEPLEARNING/TUNINGNEURALNETWORKS/NEURALNETWORKSANDDEEPLEARNING TUNINGNEURALNETWORKS 2 EXERCISE ANSWERS ##\n",
    "#### Please refer to module 1 of NeuralNetworksAndDeepLearning -TuningNeuralNetworks  for Tasks 1-9\n",
    "#### Task 1 \n",
    "##### Load the libraries that are used in this module.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a38b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper packages.\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Scikit-learn packages.\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "# TensorFlow and supporting packages.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import neptune.new as neptune\n",
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe32598",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Set the working directory to the data directory.\n",
    "##### Print the working directory.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'main_dir' to location of the project folder\n",
    "from pathlib import Path \n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ee472a",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Load the dataset `bank_marketing.csv` and save it to `bank_marketing`.\n",
    "##### Print the first few rows of `bank_marketing`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_marketing = pd.read_csv(data_dir + \"/bank_marketing.csv\")\n",
    "bank_marketing.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a68647",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Define a convenience function `ex_data_prep` to perform the data cleaning steps mentioned below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Replace the column `y` in the dataframe, by setting it to 1 if `y` is 'yes', otherwise set `y` to 0.\n",
    "2. Replace the missing values in variable `pdays` with the mean value of the column\n",
    "3. Perform one hot encoding on the variables with data type object (i.e `job`, `marital`, `education`, `default`, `housing`, `loan`, `contact`, `month`, `day_of_week` and `poutcome`) except the target variable `y`\n",
    "4. Drop the original variables and concatenate the dummies to original datset\n",
    "5. Select the predictors by dropping variable `y` and save the result to a dataframe `X_ex`\n",
    "6. Save the target variable `y` column to `y_ex` variable\n",
    "7. Set the seed to 1\n",
    "8. Split the data into training, test, and validation sets with 70:15:15 ratio and save respective variables to `X_train_ex`, `X_test_ex`, `X_val_ex`, `y_train_ex`, `y_test_ex`, `y_val_ex`\n",
    "9. Scale the train, test and the validation datasets using Min max scaler and save as `X_train_scaled_ex`, `X_test_scaled_ex` and `X_val_scaled_ex` respectiely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a833891",
   "metadata": {},
   "source": [
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_data_prep(df):\n",
    "    \n",
    "    # Convert `y` to 0/1 values\n",
    "    df['y'] = np.where(df['y'] == 'yes', 1, 0)\n",
    "    \n",
    "    # Replace missing vbalues in `pdays`\n",
    "    df = df.fillna(df.mean()['pdays'])\n",
    "    \n",
    "    # Perform one hot encoding\n",
    "    job_dummy = pd.get_dummies(df['job'], prefix = 'job', drop_first = True)\n",
    "    marital_dummy = pd.get_dummies(df['marital'], prefix = 'marital', drop_first = True)\n",
    "    education_dummy = pd.get_dummies(df['education'], prefix = 'education', drop_first = True)\n",
    "    default_dummy = pd.get_dummies(df['default'], prefix = 'default', drop_first = True)\n",
    "    housing_dummy = pd.get_dummies(df['housing'], prefix = 'housing', drop_first = True)\n",
    "    loan_dummy = pd.get_dummies(df['loan'], prefix = 'loan', drop_first = True)\n",
    "    contact_dummy = pd.get_dummies(df['contact'], prefix = 'contact', drop_first = True)\n",
    "    month_dummy = pd.get_dummies(df['month'], prefix = 'month', drop_first = True)\n",
    "    day_of_week_dummy = pd.get_dummies(df['day_of_week'], prefix = 'day_of_week', drop_first = True)\n",
    "    poutcome_dummy = pd.get_dummies(df['poutcome'], prefix = 'poutcome', drop_first = True)\n",
    "    \n",
    "    # Drop the original variables \n",
    "    df.drop(['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', \n",
    "                    'poutcome'], axis = 1, inplace = True)\n",
    "    \n",
    "    #Concatenate the dummies to original dataset\n",
    "    df = pd.concat([df,job_dummy,marital_dummy,education_dummy,default_dummy,housing_dummy,loan_dummy\n",
    "                            ,contact_dummy,month_dummy,day_of_week_dummy,poutcome_dummy], axis=1)\n",
    "    \n",
    "    # Separate predictors from target variable.\n",
    "    X_ex = df.drop(['y'], axis=1)\n",
    "    y_ex = df['y']\n",
    "    \n",
    "    # Set the seed to 1.\n",
    "    np.random.seed(1)\n",
    "    # Split data into train, test, and validation set, use a 70 - 15 - 15 split.\n",
    "    # First split data into train-test with 70% for train and 30% for test.\n",
    "    X_train_ex, X_test_ex, y_train_ex, y_test_ex = train_test_split(X_ex.values,\n",
    "                                                    y_ex,\n",
    "                                                    test_size = .3,\n",
    "                                                    random_state = 1)\n",
    "    # Then split the test data into two halves: test and validation. \n",
    "    X_test_ex, X_val_ex, y_test_ex, y_val_ex = train_test_split(X_test_ex,\n",
    "                                                y_test_ex,\n",
    "                                                test_size = .5,\n",
    "                                                random_state = 1)\n",
    "    print(\"Train shape:\", X_train_ex.shape, \"Test shape:\", X_test_ex.shape, \"Val shape:\", X_val_ex.shape)\n",
    "    \n",
    "    # Transforms features by scaling each feature to a given range.\n",
    "    # The default is the range between 0 and 1.\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train_scaled_ex = min_max_scaler.fit_transform(X_train_ex)\n",
    "    X_test_scaled_ex = min_max_scaler.transform(X_test_ex)\n",
    "    X_val_scaled_ex = min_max_scaler.transform(X_val_ex)\n",
    "    \n",
    "    return X_train_scaled_ex, X_test_scaled_ex, X_val_scaled_ex, y_train_ex, y_test_ex, y_val_ex\n",
    "  \n",
    "X_train_scaled_ex, X_test_scaled_ex, X_val_scaled_ex, y_train_ex, y_test_ex, y_val_ex = ex_data_prep(bank_marketing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7134dc39",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Define a list `METRICS` with the performance metrics - true positives, false positives, true negatives, false negatives, accuracy, precision, recall and auc.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d034577c",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Define the function to create a simple sequential neural network model with 32 neurons for the 1st hidden layer, 32 neurons for the second layer, and appropriate input and output layers, default learning rate as 0.75 and name the model `ex_create_model`\n",
    "##### Compile the model using \"adam\" optimizer, \"binary_crossentropy\" loss, and using `METRICS `list as metrics\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compile the model\n",
    "def ex_create_model(lr = .75):\n",
    "    \n",
    "    # Let's set the seed so that we can reproduce the results.\n",
    "    tf.random.set_seed(1)\n",
    "    opt = Adam(learning_rate = lr) # <- set optimizer\n",
    "    model = Sequential([\n",
    "          Dense(32, activation='relu', input_dim = X_train_scaled_ex.shape[1]),#<- set input and 1st hidden layer\n",
    "          Dense(32, activation='relu'),              #<- set 2nd hidden layer\n",
    "          Dense(1, activation='sigmoid')             #<- set output layer\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer = opt,            #<- set optimizer\n",
    "                  loss='binary_crossentropy', #<- set loss function to binary_crossentropy\n",
    "                  metrics= METRICS)           #<- set performance metric\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede87fd",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Create a new Exercise project in your Neptune account\n",
    "##### Initialize Neptune client using the init function\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ef9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init(project='USER_NAME/PROJECT_NAME',\n",
    "             api_token = 'API_TOKEN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d36376",
   "metadata": {},
   "source": [
    "#### Task 8\n",
    "##### Create a Neptune callback and save it as `neptune_callback`; add it to `callbacks` list.\n",
    "##### Using `ex_create_model` function defined above, instantiate a keras model, and save it as `model`.\n",
    "##### Fit the model using the same parameters as above, but also adding the `callbacks` argument to it.\n",
    "##### Go to Neptune website to check the result\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02574a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neptune callback function and add to a list of callbacks.\n",
    "neptune_callback = NeptuneCallback(run=run)\n",
    "callbacks = [neptune_callback]\n",
    "# Create and compile the base model.\n",
    "model = ex_create_model()\n",
    "model_default_ex = model.fit(X_train_scaled_ex, y_train_ex,\n",
    "                                epochs=25,\n",
    "                                validation_data=(X_val_scaled_ex, y_val_ex),\n",
    "                                callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173970d7",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "##### Predict and evaluate the model on test data. Save the result in the variable `result`.\n",
    "##### Print the result.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00656b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(X_test_scaled_ex, y_test_ex)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111f7f8",
   "metadata": {},
   "source": [
    "#### Please refer to module 2 of NeuralNetworksAndDeepLearning-TuningNeuralNetworks for Tasks 10-14\n",
    "#### Task 10\n",
    "##### Define the function `ex_tune_model` to tune the optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac2d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. activation function among `softmax, softplus, softsign, relu, tanh, sigmoid, hard_sigmoid, linear`\n",
    "2. number of neurons with the min_value as 8, max_value as 128 and step size as 16 in all hidden layers.\n",
    "3. Add a dropout layer with the min_value set to 0.0, max_value as 0.5, default value as 0.25 and step size of 0.05.\n",
    "4. Optimizer among `adam, sgd, rmsprop`.\n",
    "5. Learning rate among 1e-2, 1e-3, 1e-4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734fd3f2",
   "metadata": {},
   "source": [
    "##### Compile the model and set the metric to `accuracy`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa17bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_tune_model(hp):\n",
    "    \n",
    "    units = hp.Int('units',\n",
    "                  min_value=8,\n",
    "                  max_value=128,\n",
    "                  step=16)\n",
    "    \n",
    "    activation = hp.Choice('activation',\n",
    "                            [\n",
    "                              'softmax','softplus','softsign','relu',\n",
    "                              'tanh','sigmoid','hard_sigmoid','linear'\n",
    "                            ])\n",
    "    \n",
    "    dropout_1 = hp.Float('dropout_1',\n",
    "                        min_value=0.0,\n",
    "                        max_value=0.5,\n",
    "                        default=0.25,\n",
    "                        step=0.05)\n",
    "    \n",
    "    optimizer = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
    "    \n",
    "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
    "            \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(keras.layers.Dense(units=units,\n",
    "                                 activation=activation,\n",
    "                                 input_dim = X_train_scaled_ex.shape[1]))\n",
    "            \n",
    "    model.add(keras.layers.Dense(units=units,\n",
    "                                 activation=activation))\n",
    "            \n",
    "    model.add(Dropout(rate=dropout_1))\n",
    "            \n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9fa3de",
   "metadata": {},
   "source": [
    "#### Task 11\n",
    "##### Set the MAX_TRIALS to 10 and EXECUTIONS_PER_TRIAL to 5.\n",
    "##### Define the Random search tuner.\n",
    "##### Set the directory and project_name to `ex_final_tuned_model`.\n",
    "##### Look up the search space and tune the model using the search function.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TRIALS = 10\n",
    "EXECUTIONS_PER_TRIAL = 5\n",
    "tuner = RandomSearch(\n",
    "    ex_tune_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=MAX_TRIALS,\n",
    "    executions_per_trial=EXECUTIONS_PER_TRIAL,\n",
    "    directory='ex_final_tuned_model',\n",
    "    project_name='ex_final_tuned_model',\n",
    "    seed=1    \n",
    ")\n",
    "tuner.search_space_summary()\n",
    "tuner.search(x=X_train_scaled_ex,\n",
    "             y=y_train_ex,\n",
    "             epochs=25,\n",
    "             validation_data=(X_val_scaled_ex, y_val_ex))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56acfba7",
   "metadata": {},
   "source": [
    "#### Task 12\n",
    "##### View the optimal parameters using get_best_trials.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f0785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_model_result = tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "opt_model_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a0642a",
   "metadata": {},
   "source": [
    "#### Task 13\n",
    "##### Define the function `ex_create_optimized_model` and set the parameters to optimal values obtained earlier.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ebb0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_create_optimized_model(units, \n",
    "                       activation, \n",
    "                       dropout_1,\n",
    "                       optimizer,\n",
    "                       learning_rate,\n",
    "                       dropout_seed = 1):\n",
    "    \n",
    "    # Set up model.\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units,\n",
    "                    input_dim = X_train_scaled_ex.shape[1],\n",
    "                    activation = activation))     \n",
    "    model.add(Dense(units,\n",
    "                    activation = activation))     \n",
    "    \n",
    "    if dropout_1 is not None:\n",
    "        model.add(Dropout(rate = dropout_1, seed = dropout_seed))\n",
    "    \n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    # Compile model.\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = METRICS)\n",
    "    return model\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72114f6",
   "metadata": {},
   "source": [
    "#### Task 14\n",
    "##### Fit the model using the same parameters as above, and also add the `callbacks` argument to it.\n",
    "##### Open Neptune and view the results.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff4a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model = ex_create_optimized_model(**opt_model_result)\n",
    "tb_model.fit(X_train_scaled_ex,\n",
    "             y_train_ex,\n",
    "             validation_data = (X_val_scaled_ex, y_val_ex),\n",
    "             epochs = 25,\n",
    "             verbose = 0,          #<- silence the epoch output in console (optional)\n",
    "             callbacks = callbacks)#<- add callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e83ed",
   "metadata": {},
   "source": [
    "#### Task 15\n",
    "##### Evaluate the optimized model on test data using `tb_model.evaluate`  and compare the results with the base model.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_model.evaluate(X_test_scaled_ex, y_test_ex)\n",
    "model.evaluate(X_test_scaled_ex, y_test_ex)\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
